{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gridworld.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHCloz1M71TU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from grid_world import standard_grid,ACTION_SPACE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiJNX5X008DG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eta=0.001 #Considered for converging condition"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRkseEGdje3d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "0d75f212-3414-4071-85de-71f5e549317f"
      },
      "source": [
        "def print_value(V,g):\n",
        "  for i in range(g.rows):\n",
        "    print(\"---------------\")\n",
        "    for j in range(g.cols):\n",
        "      v=V.get((i,j),0)\n",
        "      if v>=0:\n",
        "        print(\"%.2f|\"%v,end=\"\")\n",
        "      else:\n",
        "        print(\" %.2f|\"%v,end=\"\")\n",
        "    print(\"\")   \n",
        "\n",
        "def print_policy(P,g):\n",
        "  for i in range(g.rows):\n",
        "    print(\"-----------------\")\n",
        "    for j in range(g.cols):\n",
        "      a=P.get((i,j),' ')\n",
        "      print('%s|'% a,end=\"\")\n",
        "    print(\"\")  \n",
        "\n",
        "def itr_policy_eval_deterministic():\n",
        "  tranprob={}\n",
        "  rewards={}\n",
        "  grid=standard_grid()\n",
        "  for i in range(grid.rows):\n",
        "    for j in range(grid.cols):\n",
        "      s=(i,j)\n",
        "      if not grid.is_terminal(s):\n",
        "        for a in ACTION_SPACE:\n",
        "          s1=grid.get_next_state(s,a)\n",
        "          tranprob[(s,a,s1)]=1\n",
        "          if s1 in grid.rewards:\n",
        "            rewards[(s,a,s1)]=grid.rewards[s1]\n",
        "\n",
        "\n",
        "            \n",
        "\n",
        "  pol = {\n",
        "    (2, 0): 'U',\n",
        "    (1, 0): 'U',\n",
        "    (0, 0): 'R',\n",
        "    (0, 1): 'R',\n",
        "    (0, 2): 'R',\n",
        "    (1, 2): 'U',\n",
        "    (2, 1): 'R',\n",
        "    (2, 2): 'U',\n",
        "    (2, 3): 'L',\n",
        "  }\n",
        "  print_policy(pol, grid)\n",
        "  V={}\n",
        "  for s in grid.all_states():\n",
        "    V[s]=0\n",
        "  gamma=0.9\n",
        "  it=0\n",
        "\n",
        "\n",
        "  \n",
        "  while True:\n",
        "    bc=0\n",
        "    for s in grid.all_states():\n",
        "      if not grid.is_terminal(s):\n",
        "        old_V=V[s]\n",
        "        new_V=0\n",
        "        for a in ACTION_SPACE:\n",
        "          for s2 in grid.all_states():\n",
        "            ac_prb=1 if pol.get(s)==a else 0\n",
        "            r=rewards.get((s,a,s2),0)\n",
        "            new_V+=ac_prb*tranprob.get((s,a,s2),0)*(r+gamma*V[s2])\n",
        "        V[s]=new_V\n",
        "        bc=max(bc,np.abs(old_V-V[s]))\n",
        "    print(\"iter:\", it, \"biggest_change:\", bc)\n",
        "    print_value(V, grid)   \n",
        "    it+=1\n",
        "    # it-=1\n",
        "    if bc<eta:\n",
        "      break\n",
        "  \n",
        "print(\"\\n\") \n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  itr_policy_eval_deterministic()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "-----------------\n",
            "R|R|R| |\n",
            "-----------------\n",
            "U| |U| |\n",
            "-----------------\n",
            "U|R|U|L|\n",
            "iter: 0 biggest_change: 1.0\n",
            "---------------\n",
            "0.00|0.00|1.00|0.00|\n",
            "---------------\n",
            "0.00|0.00|0.00|0.00|\n",
            "---------------\n",
            "0.00|0.00|0.00|0.00|\n",
            "iter: 1 biggest_change: 0.9\n",
            "---------------\n",
            "0.81|0.90|1.00|0.00|\n",
            "---------------\n",
            "0.73|0.00|0.90|0.00|\n",
            "---------------\n",
            "0.00|0.00|0.81|0.00|\n",
            "iter: 2 biggest_change: 0.7290000000000001\n",
            "---------------\n",
            "0.81|0.90|1.00|0.00|\n",
            "---------------\n",
            "0.73|0.00|0.90|0.00|\n",
            "---------------\n",
            "0.66|0.73|0.81|0.73|\n",
            "iter: 3 biggest_change: 0\n",
            "---------------\n",
            "0.81|0.90|1.00|0.00|\n",
            "---------------\n",
            "0.73|0.00|0.90|0.00|\n",
            "---------------\n",
            "0.66|0.73|0.81|0.73|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRydSG9uTWyW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Windy grid world \n",
        "#iterative_policy_evaluation_probabilistic.py  code from lazy programmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRWXT_EsTjDb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "ec98720d-0315-4d1e-86ab-729bf06a4ec6"
      },
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "from grid_world import windy_grid, ACTION_SPACE\n",
        "\n",
        "SMALL_ENOUGH = 1e-3 # threshold for convergence\n",
        "\n",
        "\n",
        "def print_values(V, g):\n",
        "  for i in range(g.rows):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.cols):\n",
        "      v = V.get((i,j), 0)\n",
        "      if v >= 0:\n",
        "        print(\" %.2f|\" % v, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "def print_policy(P, g):\n",
        "  for i in range(g.rows):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.cols):\n",
        "      a = P.get((i,j), ' ')\n",
        "      print(\"  %s  |\" % a, end=\"\")\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  ### define transition probabilities and grid ###\n",
        "  # the key is (s, a, s'), the value is the probability\n",
        "  # that is, transition_probs[(s, a, s')] = p(s' | s, a)\n",
        "  # any key NOT present will considered to be impossible (i.e. probability 0)\n",
        "  # we can take this from the grid object and convert it to the format we want\n",
        "  transition_probs = {}\n",
        "\n",
        "  # to reduce the dimensionality of the dictionary, we'll use deterministic\n",
        "  # rewards, r(s, a, s')\n",
        "  # note: you could make it simpler by using r(s') since the reward doesn't\n",
        "  # actually depend on (s, a)\n",
        "  rewards = {}\n",
        "\n",
        "  grid = windy_grid()\n",
        "  for (s, a), v in grid.probs.items():\n",
        "    for s2, p in v.items():\n",
        "      transition_probs[(s, a, s2)] = p\n",
        "      rewards[(s, a, s2)] = grid.rewards.get(s2, 0)\n",
        "\n",
        "  ### probabilistic policy ###\n",
        "  policy = {\n",
        "    (2, 0): {'U': 0.5, 'R': 0.5},\n",
        "    (1, 0): {'U': 1.0},\n",
        "    (0, 0): {'R': 1.0},\n",
        "    (0, 1): {'R': 1.0},\n",
        "    (0, 2): {'R': 1.0},\n",
        "    (1, 2): {'U': 1.0},\n",
        "    (2, 1): {'R': 1.0},\n",
        "    (2, 2): {'U': 1.0},\n",
        "    (2, 3): {'L': 1.0},\n",
        "  }\n",
        "  print_policy(policy, grid)\n",
        "  # print(transition_probs)\n",
        "  # print(rewards)\n",
        "\n",
        "  # initialize V(s) = 0\n",
        "  V = {}\n",
        "  for s in grid.all_states():\n",
        "    V[s] = 0\n",
        "\n",
        "  gamma = 0.9 # discount factor\n",
        "\n",
        "  # repeat until convergence\n",
        "  it = 0\n",
        "  while True:\n",
        "    biggest_change = 0\n",
        "    for s in grid.all_states():\n",
        "      if not grid.is_terminal(s):\n",
        "        old_v = V[s]\n",
        "        new_v = 0 # we will accumulate the answer\n",
        "        for a in ACTION_SPACE:\n",
        "          for s2 in grid.all_states():\n",
        "\n",
        "            # action probability is deterministic\n",
        "            action_prob = policy[s].get(a, 0)\n",
        "            \n",
        "            # reward is a function of (s, a, s'), 0 if not specified\n",
        "            r = rewards.get((s, a, s2), 0)\n",
        "            new_v += action_prob * transition_probs.get((s, a, s2), 0) * (r + gamma * V[s2])\n",
        "\n",
        "        # after done getting the new value, update the value table\n",
        "        V[s] = new_v\n",
        "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "\n",
        "    print(\"iter:\", it, \"biggest_change:\", biggest_change)\n",
        "    print_values(V, grid)\n",
        "    it += 1\n",
        "\n",
        "    if biggest_change < SMALL_ENOUGH:\n",
        "      break\n",
        "  # print(\"V:\", V)\n",
        "  print(\"\\n\\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------\n",
            "  {'R': 1.0}  |  {'R': 1.0}  |  {'R': 1.0}  |     |\n",
            "---------------------------\n",
            "  {'U': 1.0}  |     |  {'U': 1.0}  |     |\n",
            "---------------------------\n",
            "  {'U': 0.5, 'R': 0.5}  |  {'R': 1.0}  |  {'U': 1.0}  |  {'L': 1.0}  |\n",
            "iter: 0 biggest_change: 1.0\n",
            "---------------------------\n",
            " 0.00| 0.00| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00|-0.50| 0.00|\n",
            "---------------------------\n",
            " 0.00| 0.00|-0.45| 0.00|\n",
            "iter: 1 biggest_change: 0.9\n",
            "---------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.73| 0.00|-0.05| 0.00|\n",
            "---------------------------\n",
            "-0.18|-0.41|-0.04|-0.41|\n",
            "iter: 2 biggest_change: 0.4920750000000001\n",
            "---------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.73| 0.00|-0.05| 0.00|\n",
            "---------------------------\n",
            " 0.31|-0.04|-0.04|-0.04|\n",
            "iter: 3 biggest_change: 0\n",
            "---------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.73| 0.00|-0.05| 0.00|\n",
            "---------------------------\n",
            " 0.31|-0.04|-0.04|-0.04|\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5AyC6wQ8KpT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#policy_iteration_deterministic.py reference from course on udemy by lazy programmer. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jHQKlhI-eUd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "0e2b0037-9033-4a29-f20f-76c88e851702"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "from grid_world import standard_grid, ACTION_SPACE\n",
        "# from iterative_policy_evaluation_deterministic import print_values, print_policy\n",
        "\n",
        "SMALL_ENOUGH = 1e-3\n",
        "GAMMA = 0.9\n",
        "\n",
        "\n",
        "def print_values(V, g):\n",
        "  for i in range(g.rows):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.cols):\n",
        "      v = V.get((i,j), 0)\n",
        "      if v >= 0:\n",
        "        print(\" %.2f|\" % v, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "def print_policy(P, g):\n",
        "  for i in range(g.rows):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.cols):\n",
        "      a = P.get((i,j), ' ')\n",
        "      print(\"  %s  |\" % a, end=\"\")\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "# copied from iterative_policy_evaluation\n",
        "def get_transition_probs_and_rewards(grid):\n",
        "  ### define transition probabilities and grid ###\n",
        "  # the key is (s, a, s'), the value is the probability\n",
        "  # that is, transition_probs[(s, a, s')] = p(s' | s, a)\n",
        "  # any key NOT present will considered to be impossible (i.e. probability 0)\n",
        "  transition_probs = {}\n",
        "\n",
        "  # to reduce the dimensionality of the dictionary, we'll use deterministic\n",
        "  # rewards, r(s, a, s')\n",
        "  # note: you could make it simpler by using r(s') since the reward doesn't\n",
        "  # actually depend on (s, a)\n",
        "  rewards = {}\n",
        "\n",
        "  for i in range(grid.rows):\n",
        "    for j in range(grid.cols):\n",
        "      s = (i, j)\n",
        "      if not grid.is_terminal(s):\n",
        "        for a in ACTION_SPACE:\n",
        "          s2 = grid.get_next_state(s, a)\n",
        "          transition_probs[(s, a, s2)] = 1\n",
        "          if s2 in grid.rewards:\n",
        "            rewards[(s, a, s2)] = grid.rewards[s2]\n",
        "\n",
        "  return transition_probs, rewards\n",
        "\n",
        "\n",
        "def evaluate_deterministic_policy(grid, policy):\n",
        "  # initialize V(s) = 0\n",
        "  V = {}\n",
        "  for s in grid.all_states():\n",
        "    V[s] = 0\n",
        "\n",
        "  # repeat until convergence\n",
        "  it = 0\n",
        "  while True:\n",
        "    biggest_change = 0\n",
        "    for s in grid.all_states():\n",
        "      if not grid.is_terminal(s):\n",
        "        old_v = V[s]\n",
        "        new_v = 0 # we will accumulate the answer\n",
        "        for a in ACTION_SPACE:\n",
        "          for s2 in grid.all_states():\n",
        "\n",
        "            # action probability is deterministic\n",
        "            action_prob = 1 if policy.get(s) == a else 0\n",
        "            \n",
        "            # reward is a function of (s, a, s'), 0 if not specified\n",
        "            r = rewards.get((s, a, s2), 0)\n",
        "            new_v += action_prob * transition_probs.get((s, a, s2), 0) * (r + GAMMA * V[s2])\n",
        "\n",
        "        # after done getting the new value, update the value table\n",
        "        V[s] = new_v\n",
        "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "    it += 1\n",
        "\n",
        "    if biggest_change < SMALL_ENOUGH:\n",
        "      break\n",
        "  return V\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  grid = standard_grid()\n",
        "  transition_probs, rewards = get_transition_probs_and_rewards(grid)\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid)\n",
        "\n",
        "  # state -> action\n",
        "  # we'll randomly choose an action and update as we learn\n",
        "  policy = {}\n",
        "  for s in grid.actions.keys():\n",
        "    policy[s] = np.random.choice(ACTION_SPACE)\n",
        "\n",
        "  # initial policy\n",
        "  print(\"initial policy:\")\n",
        "  print_policy(policy, grid)\n",
        "\n",
        "  # repeat until convergence - will break out when policy does not change\n",
        "  while True:\n",
        "\n",
        "    # policy evaluation step - we already know how to do this!\n",
        "    V = evaluate_deterministic_policy(grid, policy)\n",
        "\n",
        "    # policy improvement step\n",
        "    is_policy_converged = True\n",
        "    for s in grid.actions.keys():\n",
        "      old_a = policy[s]\n",
        "      new_a = None\n",
        "      best_value = float('-inf')\n",
        "\n",
        "      # loop through all possible actions to find the best current action\n",
        "      for a in ACTION_SPACE:\n",
        "        v = 0\n",
        "        for s2 in grid.all_states():\n",
        "          # reward is a function of (s, a, s'), 0 if not specified\n",
        "          r = rewards.get((s, a, s2), 0)\n",
        "          v += transition_probs.get((s, a, s2), 0) * (r + GAMMA * V[s2])\n",
        "\n",
        "        if v > best_value:\n",
        "          best_value = v\n",
        "          new_a = a\n",
        "\n",
        "      # new_a now represents the best action in this state\n",
        "      policy[s] = new_a\n",
        "      if new_a != old_a:\n",
        "        is_policy_converged = False\n",
        "\n",
        "    if is_policy_converged:\n",
        "      break\n",
        "\n",
        "  # once we're done, print the final policy and values\n",
        "  print(\"values:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"policy:\")\n",
        "  print_policy(policy, grid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rewards:\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00|-1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "initial policy:\n",
            "---------------------------\n",
            "  R  |  R  |  U  |     |\n",
            "---------------------------\n",
            "  U  |     |  D  |     |\n",
            "---------------------------\n",
            "  D  |  D  |  R  |  D  |\n",
            "values:\n",
            "---------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.73| 0.00| 0.90| 0.00|\n",
            "---------------------------\n",
            " 0.66| 0.73| 0.81| 0.73|\n",
            "policy:\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  U  |  L  |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGNvARCzJt7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# windy grid world policy_iteration_probabilistic.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TND7NfHoJxwI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "5107a915-4726-444d-e318-7dc3c6698621"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "from grid_world import windy_grid, windy_grid_penalized, ACTION_SPACE\n",
        "#from iterative_policy_evaluation import print_values, print_policy\n",
        "\n",
        "SMALL_ENOUGH = 1e-3\n",
        "GAMMA = 0.9\n",
        "\n",
        "\n",
        "def print_values(V, g):\n",
        "  for i in range(g.rows):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.cols):\n",
        "      v = V.get((i,j), 0)\n",
        "      if v >= 0:\n",
        "        print(\" %.2f|\" % v, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "def print_policy(P, g):\n",
        "  for i in range(g.rows):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.cols):\n",
        "      a = P.get((i,j), ' ')\n",
        "      print(\"  %s  |\" % a, end=\"\")\n",
        "    print(\"\")\n",
        "\n",
        "# copied from iterative_policy_evaluation\n",
        "def get_transition_probs_and_rewards(grid):\n",
        "  ### define transition probabilities and grid ###\n",
        "  # the key is (s, a, s'), the value is the probability\n",
        "  # that is, transition_probs[(s, a, s')] = p(s' | s, a)\n",
        "  # any key NOT present will considered to be impossible (i.e. probability 0)\n",
        "  transition_probs = {}\n",
        "\n",
        "  # to reduce the dimensionality of the dictionary, we'll use deterministic\n",
        "  # rewards, r(s, a, s')\n",
        "  # note: you could make it simpler by using r(s') since the reward doesn't\n",
        "  # actually depend on (s, a)\n",
        "  rewards = {}\n",
        "\n",
        "  for (s, a), v in grid.probs.items():\n",
        "    for s2, p in v.items():\n",
        "      transition_probs[(s, a, s2)] = p\n",
        "      rewards[(s, a, s2)] = grid.rewards.get(s2, 0)\n",
        "\n",
        "  return transition_probs, rewards\n",
        "\n",
        "\n",
        "def evaluate_deterministic_policy(grid, policy):\n",
        "  # initialize V(s) = 0\n",
        "  V = {}\n",
        "  for s in grid.all_states():\n",
        "    V[s] = 0\n",
        "\n",
        "  # repeat until convergence\n",
        "  it = 0\n",
        "  while True:\n",
        "    biggest_change = 0\n",
        "    for s in grid.all_states():\n",
        "      if not grid.is_terminal(s):\n",
        "        old_v = V[s]\n",
        "        new_v = 0 # we will accumulate the answer\n",
        "        for a in ACTION_SPACE:\n",
        "          for s2 in grid.all_states():\n",
        "\n",
        "            # action probability is deterministic\n",
        "            action_prob = 1 if policy.get(s) == a else 0\n",
        "            \n",
        "            # reward is a function of (s, a, s'), 0 if not specified\n",
        "            r = rewards.get((s, a, s2), 0)\n",
        "            new_v += action_prob * transition_probs.get((s, a, s2), 0) * (r + GAMMA * V[s2])\n",
        "\n",
        "        # after done getting the new value, update the value table\n",
        "        V[s] = new_v\n",
        "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "    it += 1\n",
        "\n",
        "    if biggest_change < SMALL_ENOUGH:\n",
        "      break\n",
        "  return V\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  grid = windy_grid_penalized(-0.2)\n",
        "  # grid = windy_grid()\n",
        "  transition_probs, rewards = get_transition_probs_and_rewards(grid)\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid)\n",
        "\n",
        "  # state -> action\n",
        "  # we'll randomly choose an action and update as we learn\n",
        "  policy = {}\n",
        "  for s in grid.actions.keys():\n",
        "    policy[s] = np.random.choice(ACTION_SPACE)\n",
        "\n",
        "  # initial policy\n",
        "  print(\"initial policy:\")\n",
        "  print_policy(policy, grid)\n",
        "\n",
        "  # repeat until convergence - will break out when policy does not change\n",
        "  while True:\n",
        "\n",
        "    # policy evaluation step - we already know how to do this!\n",
        "    V = evaluate_deterministic_policy(grid, policy)\n",
        "\n",
        "    # policy improvement step\n",
        "    is_policy_converged = True\n",
        "    for s in grid.actions.keys():\n",
        "      old_a = policy[s]\n",
        "      new_a = None\n",
        "      best_value = float('-inf')\n",
        "\n",
        "      # loop through all possible actions to find the best current action\n",
        "      for a in ACTION_SPACE:\n",
        "        v = 0\n",
        "        for s2 in grid.all_states():\n",
        "          # reward is a function of (s, a, s'), 0 if not specified\n",
        "          r = rewards.get((s, a, s2), 0)\n",
        "          v += transition_probs.get((s, a, s2), 0) * (r + GAMMA * V[s2])\n",
        "\n",
        "        if v > best_value:\n",
        "          best_value = v\n",
        "          new_a = a\n",
        "\n",
        "      # new_a now represents the best action in this state\n",
        "      policy[s] = new_a\n",
        "      if new_a != old_a:\n",
        "        is_policy_converged = False\n",
        "\n",
        "    if is_policy_converged:\n",
        "      break\n",
        "\n",
        "  # once we're done, print the final policy and values\n",
        "  print(\"values:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"policy:\")\n",
        "  print_policy(policy, grid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rewards:\n",
            "---------------------------\n",
            "-0.20|-0.20|-0.20| 1.00|\n",
            "---------------------------\n",
            "-0.20| 0.00|-0.20|-1.00|\n",
            "---------------------------\n",
            "-0.20|-0.20|-0.20|-0.20|\n",
            "initial policy:\n",
            "---------------------------\n",
            "  U  |  U  |  L  |     |\n",
            "---------------------------\n",
            "  U  |     |  R  |     |\n",
            "---------------------------\n",
            "  U  |  D  |  R  |  D  |\n",
            "values:\n",
            "---------------------------\n",
            " 0.43| 0.70| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.19| 0.00|-0.15| 0.00|\n",
            "---------------------------\n",
            "-0.03|-0.23|-0.34|-0.50|\n",
            "policy:\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  L  |  U  |  L  |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvkifK1PO1zd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#value_iteration.py "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oubmDUbO2vR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "a1f1de8b-fab5-45eb-95c8-6e7f87d8aeee"
      },
      "source": [
        "import numpy as np\n",
        "from grid_world import windy_grid, ACTION_SPACE\n",
        "#from iterative_policy_evaluation import print_values, print_policy\n",
        "\n",
        "\n",
        "\n",
        "def print_values(V, g):\n",
        "  for i in range(g.rows):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.cols):\n",
        "      v = V.get((i,j), 0)\n",
        "      if v >= 0:\n",
        "        print(\" %.2f|\" % v, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "def print_policy(P, g):\n",
        "  for i in range(g.rows):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.cols):\n",
        "      a = P.get((i,j), ' ')\n",
        "      print(\"  %s  |\" % a, end=\"\")\n",
        "    print(\"\")\n",
        "\n",
        "SMALL_ENOUGH = 1e-3\n",
        "GAMMA = 0.9\n",
        "\n",
        "# copied from iterative_policy_evaluation\n",
        "def get_transition_probs_and_rewards(grid):\n",
        "  ### define transition probabilities and grid ###\n",
        "  # the key is (s, a, s'), the value is the probability\n",
        "  # that is, transition_probs[(s, a, s')] = p(s' | s, a)\n",
        "  # any key NOT present will considered to be impossible (i.e. probability 0)\n",
        "  transition_probs = {}\n",
        "\n",
        "  # to reduce the dimensionality of the dictionary, we'll use deterministic\n",
        "  # rewards, r(s, a, s')\n",
        "  # note: you could make it simpler by using r(s') since the reward doesn't\n",
        "  # actually depend on (s, a)\n",
        "  rewards = {}\n",
        "\n",
        "  for (s, a), v in grid.probs.items():\n",
        "    for s2, p in v.items():\n",
        "      transition_probs[(s, a, s2)] = p\n",
        "      rewards[(s, a, s2)] = grid.rewards.get(s2, 0)\n",
        "\n",
        "  return transition_probs, rewards\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  grid = windy_grid()\n",
        "  transition_probs, rewards = get_transition_probs_and_rewards(grid)\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid)\n",
        "\n",
        "  # initialize V(s)\n",
        "  V = {}\n",
        "  states = grid.all_states()\n",
        "  for s in states:\n",
        "    V[s] = 0\n",
        "\n",
        "  # repeat until convergence\n",
        "  # V[s] = max[a]{ sum[s',r] { p(s',r|s,a)[r + gamma*V[s']] } }\n",
        "  it = 0\n",
        "  while True:\n",
        "    biggest_change = 0\n",
        "    for s in grid.all_states():\n",
        "      if not grid.is_terminal(s):\n",
        "        old_v = V[s]\n",
        "        new_v = float('-inf')\n",
        "\n",
        "        for a in ACTION_SPACE:\n",
        "          v = 0\n",
        "          for s2 in grid.all_states():\n",
        "            # reward is a function of (s, a, s'), 0 if not specified\n",
        "            r = rewards.get((s, a, s2), 0)\n",
        "            v += transition_probs.get((s, a, s2), 0) * (r + GAMMA * V[s2])\n",
        "\n",
        "          # keep v if it's better\n",
        "          if v > new_v:\n",
        "            new_v = v\n",
        "\n",
        "        V[s] = new_v\n",
        "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "\n",
        "    it += 1\n",
        "    if biggest_change < SMALL_ENOUGH:\n",
        "      break\n",
        "\n",
        "  # find a policy that leads to optimal value function\n",
        "  policy = {}\n",
        "  for s in grid.actions.keys():\n",
        "    best_a = None\n",
        "    best_value = float('-inf')\n",
        "    # loop through all possible actions to find the best current action\n",
        "    for a in ACTION_SPACE:\n",
        "      v = 0\n",
        "      for s2 in grid.all_states():\n",
        "        # reward is a function of (s, a, s'), 0 if not specified\n",
        "        r = rewards.get((s, a, s2), 0)\n",
        "        v += transition_probs.get((s, a, s2), 0) * (r + GAMMA * V[s2])\n",
        "\n",
        "      # best_a is the action associated with best_value\n",
        "      if v > best_value:\n",
        "        best_value = v\n",
        "        best_a = a\n",
        "    policy[s] = best_a\n",
        "\n",
        "  # our goal here is to verify that we get the same answer as with policy iteration\n",
        "  print(\"values:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"policy:\")\n",
        "  print_policy(policy, grid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rewards:\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00|-1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "values:\n",
            "---------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.73| 0.00| 0.48| 0.00|\n",
            "---------------------------\n",
            " 0.66| 0.59| 0.53| 0.48|\n",
            "policy:\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  D  |     |\n",
            "---------------------------\n",
            "  U  |  L  |  L  |  L  |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxzaS4jNccvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#monte_carlo.py \n",
        "#monte_carlo policy evaluation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69eQzdyegeRn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "d2f98b51-f176-4ce8-cd5c-26305ccc4b7d"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "from grid_world import standard_grid, negative_grid\n",
        "#from iterative_policy_evaluation import print_values, print_policy\n",
        "\n",
        "SMALL_ENOUGH = 1e-3\n",
        "GAMMA = 0.9\n",
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
        "\n",
        "# NOTE: this is only policy evaluation, not optimization\n",
        "\n",
        "def print_values(V, g):\n",
        "  for i in range(g.rows):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.cols):\n",
        "      v = V.get((i,j), 0)\n",
        "      if v >= 0:\n",
        "        print(\" %.2f|\" % v, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "def print_policy(P, g):\n",
        "  for i in range(g.rows):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.cols):\n",
        "      a = P.get((i,j), ' ')\n",
        "      print(\"  %s  |\" % a, end=\"\")\n",
        "    print(\"\")\n",
        "\n",
        "def play_game(grid, policy):\n",
        "  # returns a list of states and corresponding returns\n",
        "\n",
        "  # reset game to start at a random position\n",
        "  # we need to do this, because given our current deterministic policy\n",
        "  # we would never end up at certain states, but we still want to measure their value\n",
        "  start_states = list(grid.actions.keys())\n",
        "  start_idx = np.random.choice(len(start_states))\n",
        "  grid.set_state(start_states[start_idx])\n",
        "\n",
        "  s = grid.current_state()\n",
        "  states_and_rewards = [(s, 0)] # list of tuples of (state, reward)\n",
        "  while not grid.game_over():\n",
        "    a = policy[s]\n",
        "    r = grid.move(a)\n",
        "    s = grid.current_state()\n",
        "    states_and_rewards.append((s, r))\n",
        "  # calculate the returns by working backwards from the terminal state\n",
        "  G = 0\n",
        "  states_and_returns = []\n",
        "  first = True\n",
        "  for s, r in reversed(states_and_rewards):\n",
        "    # the value of the terminal state is 0 by definition\n",
        "    # we should ignore the first state we encounter\n",
        "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
        "    if first:\n",
        "      first = False\n",
        "    else:\n",
        "      states_and_returns.append((s, G))\n",
        "    G = r + GAMMA*G\n",
        "  states_and_returns.reverse() # we want it to be in order of state visited\n",
        "  return states_and_returns\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # use the standard grid again (0 for every step) so that we can compare\n",
        "  # to iterative policy evaluation\n",
        "  grid = standard_grid()\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid)\n",
        "\n",
        "  # state -> action\n",
        "  policy = {\n",
        "    (2, 0): 'U',\n",
        "    (1, 0): 'U',\n",
        "    (0, 0): 'R',\n",
        "    (0, 1): 'R',\n",
        "    (0, 2): 'R',\n",
        "    (1, 2): 'R',\n",
        "    (2, 1): 'R',\n",
        "    (2, 2): 'R',\n",
        "    (2, 3): 'U',\n",
        "  }\n",
        "\n",
        "  # initialize V(s) and returns\n",
        "  V = {}\n",
        "  returns = {} # dictionary of state -> list of returns we've received\n",
        "  states = grid.all_states()\n",
        "  for s in states:\n",
        "    if s in grid.actions:\n",
        "      returns[s] = []\n",
        "    else:\n",
        "      # terminal state or state we can't otherwise get to\n",
        "      V[s] = 0\n",
        "\n",
        "  # repeat\n",
        "  for t in range(100):\n",
        "\n",
        "    # generate an episode using pi\n",
        "    states_and_returns = play_game(grid, policy)\n",
        "    seen_states = set()\n",
        "    for s, G in states_and_returns:\n",
        "      # check if we have already seen s\n",
        "      # called \"first-visit\" MC policy evaluation\n",
        "      if s not in seen_states:\n",
        "        returns[s].append(G)\n",
        "        V[s] = np.mean(returns[s])\n",
        "        seen_states.add(s)\n",
        "\n",
        "  print(\"values:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"policy:\")\n",
        "  print_policy(policy, grid)\n",
        "  print(seen_states)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rewards:\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00|-1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "values:\n",
            "---------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.73| 0.00|-1.00| 0.00|\n",
            "---------------------------\n",
            " 0.66|-0.81|-0.90|-1.00|\n",
            "policy:\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  R  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  R  |  U  |\n",
            "{(1, 2)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSp3rpw8sTa8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#monte carlo windy grid world\n",
        "#monte_carlo_random.py\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIePUYV6sTk_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "a8f72ec3-4ca8-49cd-8ef3-6dd939da7848"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "from grid_world import standard_grid, negative_grid\n",
        "#from iterative_policy_evaluation import print_values, print_policy\n",
        "\n",
        "SMALL_ENOUGH = 1e-3\n",
        "GAMMA = 0.9\n",
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
        "\n",
        "# NOTE: this is only policy evaluation, not optimization\n",
        "\n",
        "def print_values(V, g):\n",
        "  for i in range(g.rows):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.cols):\n",
        "      v = V.get((i,j), 0)\n",
        "      if v >= 0:\n",
        "        print(\" %.2f|\" % v, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "def print_policy(P, g):\n",
        "  for i in range(g.rows):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.cols):\n",
        "      a = P.get((i,j), ' ')\n",
        "      print(\"  %s  |\" % a, end=\"\")\n",
        "    print(\"\")\n",
        "\n",
        "def random_action(a):\n",
        "  # choose given a with probability 0.5\n",
        "  # choose some other a' != a with probability 0.5/3\n",
        "  p = np.random.random()\n",
        "  if p < 0.5:\n",
        "    return a\n",
        "  else:\n",
        "    tmp = list(ALL_POSSIBLE_ACTIONS)\n",
        "    tmp.remove(a)\n",
        "    return np.random.choice(tmp)\n",
        "\n",
        "def play_game(grid, policy):\n",
        "  # returns a list of states and corresponding returns\n",
        "\n",
        "  # reset game to start at a random position\n",
        "  # we need to do this, because given our current deterministic policy\n",
        "  # we would never end up at certain states, but we still want to measure their value\n",
        "  start_states = list(grid.actions.keys())\n",
        "  start_idx = np.random.choice(len(start_states))\n",
        "  grid.set_state(start_states[start_idx])\n",
        "\n",
        "  s = grid.current_state()\n",
        "  states_and_rewards = [(s, 0)] # list of tuples of (state, reward)\n",
        "  while not grid.game_over():\n",
        "    a = policy[s]\n",
        "    a = random_action(a)\n",
        "    r = grid.move(a)\n",
        "    s = grid.current_state()\n",
        "    states_and_rewards.append((s, r))\n",
        "  # calculate the returns by working backwards from the terminal state\n",
        "  G = 0\n",
        "  states_and_returns = []\n",
        "  first = True\n",
        "  for s, r in reversed(states_and_rewards):\n",
        "    # the value of the terminal state is 0 by definition\n",
        "    # we should ignore the first state we encounter\n",
        "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
        "    if first:\n",
        "      first = False\n",
        "    else:\n",
        "      states_and_returns.append((s, G))\n",
        "    G = r + GAMMA*G\n",
        "  states_and_returns.reverse() # we want it to be in order of state visited\n",
        "  return states_and_returns\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # use the standard grid again (0 for every step) so that we can compare\n",
        "  # to iterative policy evaluation\n",
        "  grid = standard_grid()\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid)\n",
        "\n",
        "  # state -> action\n",
        "  # found by policy_iteration_random on standard_grid\n",
        "  # MC method won't get exactly this, but should be close\n",
        "  # values:\n",
        "  # ---------------------------\n",
        "  #  0.43|  0.56|  0.72|  0.00|\n",
        "  # ---------------------------\n",
        "  #  0.33|  0.00|  0.21|  0.00|\n",
        "  # ---------------------------\n",
        "  #  0.25|  0.18|  0.11| -0.17|\n",
        "  # policy:\n",
        "  # ---------------------------\n",
        "  #   R  |   R  |   R  |      |\n",
        "  # ---------------------------\n",
        "  #   U  |      |   U  |      |\n",
        "  # ---------------------------\n",
        "  #   U  |   L  |   U  |   L  |\n",
        "  policy = {\n",
        "    (2, 0): 'U',\n",
        "    (1, 0): 'U',\n",
        "    (0, 0): 'R',\n",
        "    (0, 1): 'R',\n",
        "    (0, 2): 'R',\n",
        "    (1, 2): 'U',\n",
        "    (2, 1): 'L',\n",
        "    (2, 2): 'U',\n",
        "    (2, 3): 'L',\n",
        "  }\n",
        "\n",
        "  # initialize V(s) and returns\n",
        "  V = {}\n",
        "  returns = {} # dictionary of state -> list of returns we've received\n",
        "  states = grid.all_states()\n",
        "  for s in states:\n",
        "    if s in grid.actions:\n",
        "      returns[s] = []\n",
        "    else:\n",
        "      # terminal state or state we can't otherwise get to\n",
        "      V[s] = 0\n",
        "\n",
        "  # repeat until convergence\n",
        "  for t in range(5000):\n",
        "\n",
        "    # generate an episode using pi\n",
        "    states_and_returns = play_game(grid, policy)\n",
        "    seen_states = set()\n",
        "    for s, G in states_and_returns:\n",
        "      # check if we have already seen s\n",
        "      # called \"first-visit\" MC policy evaluation\n",
        "      if s not in seen_states:\n",
        "        returns[s].append(G)\n",
        "        V[s] = np.mean(returns[s])\n",
        "        seen_states.add(s)\n",
        "\n",
        "  print(\"values:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"policy:\")\n",
        "  print_policy(policy, grid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rewards:\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00|-1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "values:\n",
            "---------------------------\n",
            " 0.43| 0.56| 0.72| 0.00|\n",
            "---------------------------\n",
            " 0.32| 0.00| 0.18| 0.00|\n",
            "---------------------------\n",
            " 0.25| 0.18| 0.10|-0.14|\n",
            "policy:\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  U  |     |\n",
            "---------------------------\n",
            "  U  |  L  |  U  |  L  |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hBVysJswv-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcirqZvTwwCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Monte Carlo Control with exploring starts\n",
        "#monte_carlo_es.py \n",
        "#with exploring strats\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a2j0vijw-fv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 962
        },
        "outputId": "47557d30-d12f-41ca-e4fd-a68bc93226c2"
      },
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from grid_world import standard_grid, negative_grid\n",
        "#from iterative_policy_evaluation import print_values, print_policy\n",
        "\n",
        "GAMMA = 0.9\n",
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
        "\n",
        "\n",
        "def print_values(V, g):\n",
        "  for i in range(g.rows):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.cols):\n",
        "      v = V.get((i,j), 0)\n",
        "      if v >= 0:\n",
        "        print(\" %.2f|\" % v, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "def print_policy(P, g):\n",
        "  for i in range(g.rows):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.cols):\n",
        "      a = P.get((i,j), ' ')\n",
        "      print(\"  %s  |\" % a, end=\"\")\n",
        "    print(\"\")\n",
        "# NOTE: this script implements the Monte Carlo Exploring-Starts method\n",
        "#       for finding the optimal policy\n",
        "\n",
        "def play_game(grid, policy):\n",
        "  # returns a list of states and corresponding returns\n",
        "\n",
        "  # reset game to start at a random position\n",
        "  # we need to do this if we have a deterministic policy\n",
        "  # we would never end up at certain states, but we still want to measure their value\n",
        "  # this is called the \"exploring starts\" method\n",
        "  start_states = list(grid.actions.keys())\n",
        "  start_idx = np.random.choice(len(start_states))\n",
        "  grid.set_state(start_states[start_idx])\n",
        "\n",
        "  s = grid.current_state()\n",
        "  a = np.random.choice(ALL_POSSIBLE_ACTIONS) # first action is uniformly random\n",
        "\n",
        "  # be aware of the timing\n",
        "  # each triple is s(t), a(t), r(t)\n",
        "  # but r(t) results from taking action a(t-1) from s(t-1) and landing in s(t)\n",
        "  states_actions_rewards = [(s, a, 0)]\n",
        "  seen_states = set()\n",
        "  seen_states.add(grid.current_state())\n",
        "  num_steps = 0\n",
        "  while True:\n",
        "    r = grid.move(a)\n",
        "    num_steps += 1\n",
        "    s = grid.current_state()\n",
        "\n",
        "    if s in seen_states:\n",
        "      # hack so that we don't end up in an infinitely long episode\n",
        "      # bumping into the wall repeatedly\n",
        "      # if num_steps == 1 -> bumped into a wall and haven't moved anywhere\n",
        "      #   reward = -10\n",
        "      # else:\n",
        "      #   reward = falls off by 1 / num_steps\n",
        "      reward = -10. / num_steps\n",
        "      states_actions_rewards.append((s, None, reward))\n",
        "      break\n",
        "    elif grid.game_over():\n",
        "      states_actions_rewards.append((s, None, r))\n",
        "      break\n",
        "    else:\n",
        "      a = policy[s]\n",
        "      states_actions_rewards.append((s, a, r))\n",
        "    seen_states.add(s)\n",
        "\n",
        "  # calculate the returns by working backwards from the terminal state\n",
        "  G = 0\n",
        "  states_actions_returns = []\n",
        "  first = True\n",
        "  for s, a, r in reversed(states_actions_rewards):\n",
        "    # the value of the terminal state is 0 by definition\n",
        "    # we should ignore the first state we encounter\n",
        "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
        "    if first:\n",
        "      first = False\n",
        "    else:\n",
        "      states_actions_returns.append((s, a, G))\n",
        "    G = r + GAMMA*G\n",
        "  states_actions_returns.reverse() # we want it to be in order of state visited\n",
        "  return states_actions_returns\n",
        "\n",
        "\n",
        "def max_dict(d):\n",
        "  # returns the argmax (key) and max (value) from a dictionary\n",
        "  # put this into a function since we are using it so often\n",
        "  max_key = None\n",
        "  max_val = float('-inf')\n",
        "  for k, v in d.items():\n",
        "    if v > max_val:\n",
        "      max_val = v\n",
        "      max_key = k\n",
        "  return max_key, max_val\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # use the standard grid again (0 for every step) so that we can compare\n",
        "  # to iterative policy evaluation\n",
        "  # grid = standard_grid()\n",
        "  # try the negative grid too, to see if agent will learn to go past the \"bad spot\"\n",
        "  # in order to minimize number of steps\n",
        "  # grid = negative_grid(step_cost=-0.9)\n",
        "  grid=standard_grid()\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid)\n",
        "\n",
        "  # state -> action\n",
        "  # initialize a random policy\n",
        "  policy = {}\n",
        "  for s in grid.actions.keys():\n",
        "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
        "\n",
        "  # initialize Q(s,a) and returns\n",
        "  Q = {}\n",
        "  returns = {} # dictionary of state -> list of returns we've received\n",
        "  states = grid.all_states()\n",
        "  for s in states:\n",
        "    if s in grid.actions: # not a terminal state\n",
        "      Q[s] = {}\n",
        "      for a in ALL_POSSIBLE_ACTIONS:\n",
        "        Q[s][a] = 0 # needs to be initialized to something so we can argmax it\n",
        "        returns[(s,a)] = []\n",
        "    else:\n",
        "      # terminal state or state we can't otherwise get to\n",
        "      pass\n",
        "\n",
        "  # repeat until convergence\n",
        "  deltas = []\n",
        "  for t in range(2000):\n",
        "    if t % 100 == 0:\n",
        "      print(t)\n",
        "\n",
        "    # generate an episode using pi\n",
        "    biggest_change = 0\n",
        "    states_actions_returns = play_game(grid, policy)\n",
        "    seen_state_action_pairs = set()\n",
        "    for s, a, G in states_actions_returns:\n",
        "      # check if we have already seen s\n",
        "      # called \"first-visit\" MC policy evaluation\n",
        "      sa = (s, a)\n",
        "      if sa not in seen_state_action_pairs:\n",
        "        old_q = Q[s][a]\n",
        "        returns[sa].append(G)\n",
        "        Q[s][a] = np.mean(returns[sa])\n",
        "        biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
        "        seen_state_action_pairs.add(sa)\n",
        "    deltas.append(biggest_change)\n",
        "\n",
        "    # update policy\n",
        "    for s in policy.keys():\n",
        "      policy[s] = max_dict(Q[s])[0]\n",
        "\n",
        "  plt.plot(deltas)\n",
        "  plt.show()\n",
        "\n",
        "  print(\"final policy:\")\n",
        "  print_policy(policy, grid)\n",
        "\n",
        "  # find V\n",
        "  V = {}\n",
        "  for s, Qs in Q.items():\n",
        "    V[s] = max_dict(Q[s])[1]\n",
        "\n",
        "  print(\"final values:\")\n",
        "  print_values(V, grid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rewards:\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00|-1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "final policy:\n",
            "---------------------------\n",
            "  D  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  D  |     |  U  |     |\n",
            "---------------------------\n",
            "  R  |  R  |  U  |  L  |\n",
            "final values:\n",
            "---------------------------\n",
            "-1.01|-0.91| 1.00| 0.00|\n",
            "---------------------------\n",
            "-0.88| 0.00| 0.21| 0.00|\n",
            "---------------------------\n",
            "-0.73|-0.61|-0.08|-0.91|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDBCzfAdZ-iO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#monte carlo control without exploring start"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4CEGLfAaxCL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "outputId": "fe47bc30-22af-49fa-c477-c440b708f174"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from grid_world import standard_grid, negative_grid\n",
        "#from iterative_policy_evaluation import print_values, print_policy\n",
        "#from monte_carlo_es import max_dict\n",
        "\n",
        "GAMMA = 0.9\n",
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
        "\n",
        "# NOTE: find optimal policy and value function\n",
        "#       using on-policy first-visit MC\n",
        "\n",
        "def random_action(a, eps=0.1):\n",
        "  # choose given a with probability 1 - eps + eps/4\n",
        "  # choose some other a' != a with probability eps/4\n",
        "  p = np.random.random()\n",
        "  # if p < (1 - eps + eps/len(ALL_POSSIBLE_ACTIONS)):\n",
        "  #   return a\n",
        "  # else:\n",
        "  #   tmp = list(ALL_POSSIBLE_ACTIONS)\n",
        "  #   tmp.remove(a)\n",
        "  #   return np.random.choice(tmp)\n",
        "  #\n",
        "  # this is equivalent to the above\n",
        "  if p < (1 - eps):\n",
        "    return a\n",
        "  else:\n",
        "    return np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
        "\n",
        "def play_game(grid, policy):\n",
        "  # returns a list of states and corresponding returns\n",
        "  # in this version we will NOT use \"exploring starts\" method\n",
        "  # instead we will explore using an epsilon-soft policy\n",
        "  s = (2, 0)\n",
        "  grid.set_state(s)\n",
        "  a = random_action(policy[s])\n",
        "\n",
        "  # be aware of the timing\n",
        "  # each triple is s(t), a(t), r(t)\n",
        "  # but r(t) results from taking action a(t-1) from s(t-1) and landing in s(t)\n",
        "  states_actions_rewards = [(s, a, 0)]\n",
        "  while True:\n",
        "    r = grid.move(a)\n",
        "    s = grid.current_state()\n",
        "    if grid.game_over():\n",
        "      states_actions_rewards.append((s, None, r))\n",
        "      break\n",
        "    else:\n",
        "      a = random_action(policy[s]) # the next state is stochastic\n",
        "      states_actions_rewards.append((s, a, r))\n",
        "\n",
        "  # calculate the returns by working backwards from the terminal state\n",
        "  G = 0\n",
        "  states_actions_returns = []\n",
        "  first = True\n",
        "  for s, a, r in reversed(states_actions_rewards):\n",
        "    # the value of the terminal state is 0 by definition\n",
        "    # we should ignore the first state we encounter\n",
        "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
        "    if first:\n",
        "      first = False\n",
        "    else:\n",
        "      states_actions_returns.append((s, a, G))\n",
        "    G = r + GAMMA*G\n",
        "  states_actions_returns.reverse() # we want it to be in order of state visited\n",
        "  return states_actions_returns\n",
        "\n",
        "def print_values(V, g):\n",
        "  for i in range(g.rows):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.cols):\n",
        "      v = V.get((i,j), 0)\n",
        "      if v >= 0:\n",
        "        print(\" %.2f|\" % v, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "def print_policy(P, g):\n",
        "  for i in range(g.rows):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.cols):\n",
        "      a = P.get((i,j), ' ')\n",
        "      print(\"  %s  |\" % a, end=\"\")\n",
        "    print(\"\")\n",
        "\n",
        "def max_dict(d):\n",
        "  # returns the argmax (key) and max (value) from a dictionary\n",
        "  # put this into a function since we are using it so often\n",
        "  max_key = None\n",
        "  max_val = float('-inf')\n",
        "  for k, v in d.items():\n",
        "    if v > max_val:\n",
        "      max_val = v\n",
        "      max_key = k\n",
        "  return max_key, max_val\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # use the standard grid again (0 for every step) so that we can compare\n",
        "  # to iterative policy evaluation\n",
        "  # grid = standard_grid()\n",
        "  # try the negative grid too, to see if agent will learn to go past the \"bad spot\"\n",
        "  # in order to minimize number of steps\n",
        "  grid = negative_grid(step_cost=-0.1)\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid)\n",
        "\n",
        "  # state -> action\n",
        "  # initialize a random policy\n",
        "  policy = {}\n",
        "  for s in grid.actions.keys():\n",
        "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
        "\n",
        "  # initialize Q(s,a) and returns\n",
        "  Q = {}\n",
        "  returns = {} # dictionary of state -> list of returns we've received\n",
        "  states = grid.all_states()\n",
        "  for s in states:\n",
        "    if s in grid.actions: # not a terminal state\n",
        "      Q[s] = {}\n",
        "      for a in ALL_POSSIBLE_ACTIONS:\n",
        "        Q[s][a] = 0\n",
        "        returns[(s,a)] = []\n",
        "    else:\n",
        "      # terminal state or state we can't otherwise get to\n",
        "      pass\n",
        "\n",
        "  # repeat until convergence\n",
        "  deltas = []\n",
        "  for t in range(5000):\n",
        "    if t % 1000 == 0:\n",
        "      print(t)\n",
        "\n",
        "    # generate an episode using pi\n",
        "    biggest_change = 0\n",
        "    states_actions_returns = play_game(grid, policy)\n",
        "\n",
        "    # calculate Q(s,a)\n",
        "    seen_state_action_pairs = set()\n",
        "    for s, a, G in states_actions_returns:\n",
        "      # check if we have already seen s\n",
        "      # called \"first-visit\" MC policy evaluation\n",
        "      sa = (s, a)\n",
        "      if sa not in seen_state_action_pairs:\n",
        "        old_q = Q[s][a]\n",
        "        returns[sa].append(G)\n",
        "        Q[s][a] = np.mean(returns[sa])\n",
        "        biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
        "        seen_state_action_pairs.add(sa)\n",
        "    deltas.append(biggest_change)\n",
        "\n",
        "    # calculate new policy pi(s) = argmax[a]{ Q(s,a) }\n",
        "    for s in policy.keys():\n",
        "      a, _ = max_dict(Q[s])\n",
        "      policy[s] = a\n",
        "\n",
        "  plt.plot(deltas)\n",
        "  plt.show()\n",
        "\n",
        "  # find the optimal state-value function\n",
        "  # V(s) = max[a]{ Q(s,a) }\n",
        "  V = {}\n",
        "  for s in policy.keys():\n",
        "    V[s] = max_dict(Q[s])[1]\n",
        "\n",
        "  print(\"final values:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"final policy:\")\n",
        "  print_policy(policy, grid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rewards:\n",
            "---------------------------\n",
            "-0.10|-0.10|-0.10| 1.00|\n",
            "---------------------------\n",
            "-0.10| 0.00|-0.10|-1.00|\n",
            "---------------------------\n",
            "-0.10|-0.10|-0.10|-0.10|\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "final values:\n",
            "---------------------------\n",
            "-0.33| 0.78| 1.00| 0.00|\n",
            "---------------------------\n",
            " 0.06| 0.00| 0.77| 0.00|\n",
            "---------------------------\n",
            " 0.22| 0.37| 0.54| 0.33|\n",
            "final policy:\n",
            "---------------------------\n",
            "  D  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  D  |     |  U  |     |\n",
            "---------------------------\n",
            "  R  |  R  |  U  |  L  |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbmnW1aXmHuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlI6G6prmH9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#td0_prediction.py\n",
        "#TD(0) implementation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV3MdkUIZyPT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "94cafaef-7813-4430-8d21-943dc1d1c6b0"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from grid_world import standard_grid, negative_grid\n",
        "#from iterative_policy_evaluation import print_values, print_policy\n",
        "\n",
        "SMALL_ENOUGH = 1e-3\n",
        "GAMMA = 0.9\n",
        "ALPHA = 0.1\n",
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
        "\n",
        "# NOTE: this is only policy evaluation, not optimization\n",
        "\n",
        "def print_values(V, g):\n",
        "  for i in range(g.rows):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.cols):\n",
        "      v = V.get((i,j), 0)\n",
        "      if v >= 0:\n",
        "        print(\" %.2f|\" % v, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "def print_policy(P, g):\n",
        "  for i in range(g.rows):\n",
        "    print(\"---------------------------\")\n",
        "    for j in range(g.cols):\n",
        "      a = P.get((i,j), ' ')\n",
        "      print(\"  %s  |\" % a, end=\"\")\n",
        "    print(\"\")\n",
        "\n",
        "def random_action(a, eps=0.1):\n",
        "  # we'll use epsilon-soft to ensure all states are visited\n",
        "  # what happens if you don't do this? i.e. eps=0\n",
        "  p = np.random.random()\n",
        "  if p < (1 - eps):\n",
        "    return a\n",
        "  else:\n",
        "    return np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
        "\n",
        "def play_game(grid, policy):\n",
        "  # returns a list of states and corresponding rewards (not returns as in MC)\n",
        "  # start at the designated start state\n",
        "  s = (2, 0)\n",
        "  grid.set_state(s)\n",
        "  states_and_rewards = [(s, 0)] # list of tuples of (state, reward)\n",
        "  while not grid.game_over():\n",
        "    a = policy[s]\n",
        "    a = random_action(a)\n",
        "    r = grid.move(a)\n",
        "    s = grid.current_state()\n",
        "    states_and_rewards.append((s, r))\n",
        "  return states_and_rewards\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # use the standard grid again (0 for every step) so that we can compare\n",
        "  # to iterative policy evaluation\n",
        "  grid = standard_grid()\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid)\n",
        "\n",
        "  # state -> action\n",
        "  policy = {\n",
        "    (2, 0): 'U',\n",
        "    (1, 0): 'U',\n",
        "    (0, 0): 'R',\n",
        "    (0, 1): 'R',\n",
        "    (0, 2): 'R',\n",
        "    (1, 2): 'R',\n",
        "    (2, 1): 'R',\n",
        "    (2, 2): 'R',\n",
        "    (2, 3): 'U',\n",
        "  }\n",
        "\n",
        "  # initialize V(s) and returns\n",
        "  V = {}\n",
        "  states = grid.all_states()\n",
        "  for s in states:\n",
        "    V[s] = 0\n",
        "\n",
        "  # repeat until convergence\n",
        "  for it in range(1000):\n",
        "\n",
        "    # generate an episode using pi\n",
        "    states_and_rewards = play_game(grid, policy)\n",
        "    # the first (s, r) tuple is the state we start in and 0\n",
        "    # (since we don't get a reward) for simply starting the game\n",
        "    # the last (s, r) tuple is the terminal state and the final reward\n",
        "    # the value for the terminal state is by definition 0, so we don't\n",
        "    # care about updating it.\n",
        "    for t in range(len(states_and_rewards) - 1):\n",
        "      s, _ = states_and_rewards[t]\n",
        "      s2, r = states_and_rewards[t+1]\n",
        "      # we will update V(s) AS we experience the episode\n",
        "      V[s] = V[s] + ALPHA*(r + GAMMA*V[s2] - V[s])\n",
        "\n",
        "  print(\"values:\")\n",
        "  print_values(V, grid)\n",
        "  print(\"policy:\")\n",
        "  print_policy(policy, grid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rewards:\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00|-1.00|\n",
            "---------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "values:\n",
            "---------------------------\n",
            " 0.79| 0.88| 0.99| 0.00|\n",
            "---------------------------\n",
            " 0.70| 0.00|-0.90| 0.00|\n",
            "---------------------------\n",
            " 0.61|-0.36|-0.63|-0.88|\n",
            "policy:\n",
            "---------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "---------------------------\n",
            "  U  |     |  R  |     |\n",
            "---------------------------\n",
            "  U  |  R  |  R  |  U  |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSZeS1P7pM1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LtnHdTfpM_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYjECjB5pNNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7slyC0TMpNSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAsn7DxvpNdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAQKGSkipNmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZyYOVaipNqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4xC3qm7pNjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuol1FZRpNhZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa_raKIopNaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCjXxWvxpNYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPxJcfI4pNK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0tAnDRqpNH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy7OI5KepNFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfjlKHZepNCn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uct4TAfepM9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk4ftiSWToMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}